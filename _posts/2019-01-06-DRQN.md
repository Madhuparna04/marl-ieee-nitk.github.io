---
layout: post
title:  "Deep Recurrent Q-Network"
date:   2019-01-06 11:42:04 +0530
categories: jekyll update
author: Madhuparna Bhowmik
---

In this article, we will learn about Deep recurrent Q-learning and POMDP and find out why DRQN works better in case of POMDP than DQN.

## MDP vs. POMDP (Partially Observable MDP)

In an MDP we have full knowledge of the environment. But in most of the cases, it is not possible to know the complete state of the environment. Like a single game screen is not sufficient to determine the state of the system. For example, have a look at the picture below -

![Passing ball](/assets/passing_ball.png){:height="50%" width="50%"}


Just by looking at one image we cannot say whether the girl passed the ball and the boy is trying to catch it or vice versa. Therefore we only have partial knowledge of the environment, and this is the case most of the time. Thus for our agent to learn a policy in a POMDP where instead of entire information about the current state only an observation of the current state is available, we need to use a different method, a method such that our agent automatically can remember what it has seen in past and can learn a policy.

## Why DRQN and not DQN for POMDPs?

In the DQN algorithm, we had to stack frames together to get complete information of the current state like have a look at the images below -

![Passing ball](/assets/bp2.jpeg){:height="50%" width="50%"}

With four pictures together we can make out in which direction the ball is moving, it can also help in having a sense of velocity of the moving object. According to the [DRQN Paper](https://arxiv.org/abs/1507.06527) for Atari 2600 games, stacking four frames together makes all the games an MDP. But for more complex environments stacking four frames together may not be enough. The agent may need to remember something that happened many time steps ago to understand the current state. In that case, i.e., in case of such POMDPs DRQN works better than DQN.

In DRQN the first fully connected layer of DQN is replaced by an LSTM layer. Recurrent neural networks can remember information from several time steps before and thus LSTMs are used. This is why DRQNs can learn even if single game screens are passed one by one, and hence they work better than DQN in case of POMDPs.

Following is the DRQN architecture used in the original paper.

![Passing ball](/assets/drqn.jpeg){:height="50%" width="50%"}

## Implementation Details

Here, I am listing down some of the important points to be taken care of while implementing DRQN. For more details refer the [DRQN Paper](https://arxiv.org/abs/1507.06527).

- LSTM's hidden state must be set to zero at the start of each update. However, this makes it hard for LSTM to learn functions that span over longer timescales.

- Stable recurrent updates

	- Bootstrapped Sequential Updates

		- In this, the episodes are selected randomly from the replay memory, and updates proceed from the beginning of the episode to the end of it. The RNNs hidden state is carried forward throughout the episodes.

	- Bootstrapped Random Updates

		- Episodes are selected randomly from the replay memory, and updates start at a random point in the episode and continue for a certain number of time steps.

- Both sequential updates and random updates yield convergent policies and hence any of them can be used.


